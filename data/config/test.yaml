dataset:
  json_file: data/input/sample_data.json
  csv_file: data/input/sample_targets.csv
  train_size: 0.5
  valid_size: 0.25
  test_size: 0.25
  seed: 0
  num_examples: -1
preprocess:
  max_tokens: 25000  # Limit the maximum size of the vocabulary
model:
  batch_size: 64  # Define batch size
  glove_file_path: "data/glove/glove.6B/glove.6B.300d.txt"
  word2vec_file_path: "data/glove/glove.6B/glove.6B.300d.word2vec.txt"
  embed_dim: 300
  output_dim: 4
  num_epochs: 10
  checkpoint_file: "data/checkpoint/state_dict_tmp.pt"
  predictions_file: "data/predictions/results.csv"
verbose: true