{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-label text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'dataset': {'csv_file': '../data/input/sample_targets.csv',\n",
    "             'json_file': '../data/input/sample_data.json',\n",
    "             'num_examples': 10,\n",
    "             'seed': 0,\n",
    "             'test_size': 0.25,\n",
    "             'train_size': 0.5,\n",
    "             'valid_size': 0.25},\n",
    " 'model': {'batch_size': 64,\n",
    "           'checkpoint_file': '../data/checkpoint/state_dict.pt',\n",
    "           'embed_dim': 300,\n",
    "           'glove_file_path': '../data/glove/glove.6B/glove.6B.300d.txt',\n",
    "           'num_epochs': 2,\n",
    "           'output_dim': 4,\n",
    "           'word2vec_file_path': '../data/glove/glove.6B/glove.6B.300d.word2vec.txt'},\n",
    " 'preprocess': {'max_tokens': 25000},\n",
    " 'verbose': True}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Loaded dataset with 10 samples\n",
      "\t- There are 4 different targets\n",
      "[2, 1, 3, 0]\n",
      "Dataset structure\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'submitter', 'authors', 'title', 'comments', 'journal-ref', 'doi', 'report-no', 'categories', 'license', 'abstract', 'versions', 'update_date', 'authors_parsed', 'target'],\n",
      "        num_rows: 5\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'submitter', 'authors', 'title', 'comments', 'journal-ref', 'doi', 'report-no', 'categories', 'license', 'abstract', 'versions', 'update_date', 'authors_parsed', 'target'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'submitter', 'authors', 'title', 'comments', 'journal-ref', 'doi', 'report-no', 'categories', 'license', 'abstract', 'versions', 'update_date', 'authors_parsed', 'target'],\n",
      "        num_rows: 3\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|██████████| 5/5 [00:09<00:00,  1.91s/ examples]\n",
      "num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.\n",
      "Map (num_proc=2): 100%|██████████| 2/2 [00:02<00:00,  1.44s/ examples]\n",
      "num_proc must be <= 3. Reducing num_proc to 3 for dataset of size 3.\n",
      "Map (num_proc=3): 100%|██████████| 3/3 [00:03<00:00,  1.11s/ examples]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 535\n",
      "First 10 tokens in vocabulary: ['<unk>', \"'\", ',', ':', '[', ']', '``', '.', 'none', '{', '}', \"'created\", \"'version\", 'gmt', '(', ')', 'market', 'a/b', 'data', \"''\", \"'abstract\", \"'authors\", \"'authors_parsed\", \"'categories\", \"'comments\", \"'doi\", \"'http\", \"'id\", \"'journal-ref\", \"'license\", \"'report-no\", \"'submitter\", \"'target\", \"'title\", \"'update_date\", \"'v1\", \"'versions\", '2020', 'budget-controlled', 'buyer', 'cancer', 'drug', 'optimal', 'testing', '2021', 'mechanism', 'model', 'nov', 'participation', 'response']\n",
      "* Loading glove embeddings from file: ../data/glove/glove.6B/glove.6B.300d.txt\n",
      "word2vec_file already exists: ../data/glove/glove.6B/glove.6B.300d.word2vec.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type              | Params | Mode \n",
      "--------------------------------------------------------\n",
      "0 | embedding | Embedding         | 120 M  | train\n",
      "1 | pool      | AdaptiveMaxPool1d | 0      | train\n",
      "2 | fc        | Linear            | 1.2 K  | train\n",
      "3 | loss      | CrossEntropyLoss  | 0      | train\n",
      "4 | accuracy  | BinaryAccuracy    | 0      | train\n",
      "--------------------------------------------------------\n",
      "120 M     Trainable params\n",
      "0         Non-trainable params\n",
      "120 M     Total params\n",
      "480.005   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1/1 [00:01<00:00,  0.96it/s, v_num=1, val_loss=1.430, val_accuracy=0.500]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1/1 [00:05<00:00,  0.18it/s, v_num=1, val_loss=1.430, val_accuracy=0.500]\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 111.01it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      test_accuracy         0.5833333134651184\n",
      "        test_loss           1.3729740381240845\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Saving checkpoint to ../data/checkpoint/state_dict.pt...\n"
     ]
    }
   ],
   "source": [
    "import main\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "main.training_pipeline(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multilabel_text_classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
